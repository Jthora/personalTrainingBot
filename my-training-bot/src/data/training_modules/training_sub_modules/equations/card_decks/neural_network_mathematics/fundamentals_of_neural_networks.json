{
    "id": "fundamentals_of_neural_networks",
    "name": "Fundamentals of Neural Networks",
    "description": "Understanding the core principles and mathematical foundations of neural networks.",
    "focus": ["Neural Network Basics", "Mathematical Representations"],
    "cards": [
        {
            "id": "perceptron_model",
            "title": "Perceptron Model & Linear Separability",
            "description": "Understanding the basic unit of a neural network and its ability to classify data.",
            "bulletpoints": [
                "Define the perceptron as a simple neuron model where the output is given by \\[ y = f(\\mathbf{w}^T \\mathbf{x} + b) \\]",
                "Understand the concept of linear separability and how the decision boundary is defined by \\[ \\mathbf{w}^T \\mathbf{x} + b = 0 \\]",
                "Explore how perceptrons use the dot product calculation \\[ \\sum w_i x_i \\] to determine the class of an input."
            ],
            "summaryText": "Use perceptrons to classify linearly separable data via weighted dot products and thresholds.",
            "duration": 8,
            "difficulty": "Intermediate"
        },
        {
            "id": "multi_layer_perceptron",
            "title": "Multi-Layer Perceptron (MLP) & Non-Linearity",
            "description": "Exploring how multiple layers in neural networks improve pattern recognition.",
            "bulletpoints": [
                "Understand how adding hidden layers allows networks to model complex relationships beyond linear separability.",
                "Study activation functions: \\[ \\text{Sigmoid: } \\sigma(x) = \\frac{1}{1 + e^{-x}} \\], \\[ \\text{ReLU: } f(x) = \\max(0, x) \\], \\[ \\text{Tanh: } \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\]",
                "Explore forward propagation using matrix multiplications: \\[ \\mathbf{z} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} \\]"
            ],
            "summaryText": "Stack hidden layers with nonlinear activations to capture complex patterns beyond linear separability.",
            "duration": 10,
            "difficulty": "Advanced"
        },
        {
            "id": "tensor_representations",
            "title": "Tensor Representations & Neural Network Operations",
            "description": "Understanding how tensors are used to represent inputs, weights, and activations in deep learning.",
            "bulletpoints": [
                "Learn the role of tensors in computational graphs for neural networks: \\[ T_{ijk} \\] representing 3D data structures.",
                "Explore vectorized operations for efficient computation, such as \\[ \\mathbf{A} \\cdot \\mathbf{B} \\] for matrix multiplication.",
                "Understand how matrix multiplication is used in neural network weight updates: \\[ \\Delta W = - \\eta \\frac{\\partial L}{\\partial W} \\]"
            ],
            "summaryText": "Represent data and parameters as tensors to exploit vectorized neural computations.",
            "duration": 12,
            "difficulty": "Expert"
        },
        {
            "id": "gradient_descent",
            "title": "Gradient Descent & Optimization",
            "description": "Exploring how neural networks update weights using optimization algorithms.",
            "bulletpoints": [
                "Define the gradient descent update rule: \\[ W^{(t+1)} = W^{(t)} - \\eta \\nabla L(W) \\]",
                "Explore learning rate \\( \\eta \\) and its impact on convergence speed.",
                "Understand how stochastic gradient descent (SGD) and Adam optimizer improve efficiency."
            ],
            "summaryText": "Iteratively adjust weights with gradient descent variants tuned by the learning rate.",
            "duration": 10,
            "difficulty": "Advanced"
        },
        {
            "id": "backpropagation",
            "title": "Backpropagation Algorithm",
            "description": "Understanding how neural networks learn through backpropagation.",
            "bulletpoints": [
                "Calculate error propagation using the chain rule: \\[ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial W} \\]",
                "Update weights using gradients: \\[ W^{(t+1)} = W^{(t)} - \\eta \\frac{\\partial L}{\\partial W} \\]",
                "Understand how backpropagation works layer by layer in deep networks."
            ],
            "summaryText": "Backpropagate errors with the chain rule to refine weights across layers.",
            "duration": 14,
            "difficulty": "Expert"
        },
        {
            "id": "loss_functions",
            "title": "Loss Functions & Error Measurement",
            "description": "Exploring functions used to measure the performance of neural networks.",
            "bulletpoints": [
                "Understand mean squared error (MSE): \\[ L = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2 \\]",
                "Explore cross-entropy loss for classification: \\[ L = - \\sum y_i \\log \\hat{y}_i \\]",
                "Analyze how loss functions influence weight updates during training."
            ],
            "summaryText": "Choose losses like MSE or cross-entropy to quantify prediction errors for training.",
            "duration": 10,
            "difficulty": "Advanced"
        },
        {
            "id": "activation_functions",
            "title": "Activation Functions & Their Role",
            "description": "Exploring different activation functions used in neural networks.",
            "bulletpoints": [
                "Study sigmoid function: \\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\] and its limitations.",
                "Explore ReLU (Rectified Linear Unit): \\[ f(x) = \\max(0, x) \\] and its benefits in deep learning.",
                "Understand the importance of non-linearity in neural networks."
            ],
            "summaryText": "Select activation functions to inject nonlinearity and control gradient behavior.",
            "duration": 10,
            "difficulty": "Intermediate"
        },
        {
            "id": "softmax_function",
            "title": "Softmax Function for Probabilistic Output",
            "description": "Using the softmax function to convert network outputs into probability distributions.",
            "bulletpoints": [
                "Define softmax: \\[ \\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} \\]",
                "Understand its role in multi-class classification problems.",
                "Analyze how softmax converts logits into probabilities for output interpretation."
            ],
            "summaryText": "Normalize logits with softmax so outputs form interpretable class probabilities.",
            "duration": 12,
            "difficulty": "Advanced"
        },
        {
            "id": "batch_normalization",
            "title": "Batch Normalization in Neural Networks",
            "description": "Using batch normalization to stabilize and accelerate training.",
            "bulletpoints": [
                "Normalize activations using batch mean and variance: \\[ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sigma_B} \\]",
                "Improve convergence speed by reducing internal covariate shift.",
                "Understand how normalization is applied across mini-batches."
            ],
            "summaryText": "Standardize mini-batch activations to stabilize and accelerate training.",
            "duration": 14,
            "difficulty": "Expert"
        },
        {
            "id": "dropout_regularization",
            "title": "Dropout Regularization & Overfitting Prevention",
            "description": "Preventing overfitting in deep networks using dropout techniques.",
            "bulletpoints": [
                "Define dropout: Randomly setting activations to zero during training.",
                "Mathematically expressed as: \\[ \\tilde{h}_i = h_i \\cdot r, \\quad r \\sim Bernoulli(p) \\]",
                "Analyze how dropout improves generalization in large models."
            ],
            "summaryText": "Randomly drop activations during training to reduce overfitting.",
            "duration": 12,
            "difficulty": "Advanced"
        },
        {
            "id": "neural_network_initialization",
            "title": "Weight Initialization Strategies",
            "description": "Understanding how proper weight initialization affects learning dynamics.",
            "bulletpoints": [
                "Study Xavier/Glorot initialization: \\[ W \\sim \\mathcal{N}(0, \\frac{1}{n}) \\]",
                "Explore He initialization for ReLU: \\[ W \\sim \\mathcal{N}(0, \\frac{2}{n}) \\]",
                "Analyze the impact of poor initialization on gradient flow and training speed."
            ],
            "summaryText": "Start training with Xavier or He weight scales to maintain healthy gradients.",
            "duration": 12,
            "difficulty": "Advanced"
        }
    ]
}