{
    "id": "ai_model_training_and_tuning",
    "name": "AI Model Training & Tuning",
    "description": "Optimizing neural networks through training techniques, hyperparameter tuning, and performance evaluation.",
    "focus": [
        "Gradient Descent",
        "Regularization",
        "Hyperparameter Optimization"
    ],
    "cards": [
        {
            "id": "gradient_descent",
            "title": "Gradient Descent & Backpropagation",
            "description": "Understanding how neural networks learn by adjusting weights iteratively.",
            "bulletpoints": [
                "Study the gradient descent algorithm and its role in minimizing loss functions: \\[ \\theta := \\theta - \\alpha \\nabla J(\\theta) \\]",
                "Understand the backpropagation process using the chain rule: \\[ \\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w} \\]",
                "Explore variations such as Stochastic Gradient Descent (SGD) and Adam optimizer: \\[ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\]"
            ],
            "summaryText": "Iteratively update weights via gradient descent and backprop variants to minimize loss.",
            "duration": 10,
            "difficulty": "Advanced"
        },
        {
            "id": "regularization_methods",
            "title": "Regularization Methods & Overfitting Prevention",
            "description": "Techniques to improve model generalization and prevent overfitting.",
            "bulletpoints": [
                "Apply L1 (Lasso) and L2 (Ridge) regularization: \\[ J(\\theta) = \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\]",
                "Use dropout layers to prevent co-adaptation of neurons: \\[ h^l = f(W^l h^{l-1} + b^l) \\] with random dropout of neurons.",
                "Perform cross-validation to evaluate model performance on unseen data."
            ],
            "summaryText": "Blend L1/L2 penalties, dropout, and cross-validation to keep models from overfitting.",
            "duration": 12,
            "difficulty": "Expert"
        },
        {
            "id": "hyperparameter_tuning",
            "title": "Hyperparameter Tuning & Model Selection",
            "description": "Optimizing neural network architectures by selecting the best hyperparameters.",
            "bulletpoints": [
                "Optimize learning rate \\( \\alpha \\) to balance convergence speed and stability: \\[ \\theta := \\theta - \\alpha \\nabla J(\\theta) \\]",
                "Use grid search and random search for hyperparameter selection.",
                "Explore Bayesian Optimization to model the performance function using Gaussian Processes."
            ],
            "summaryText": "Search learning rates and other hyperparameters with grid, random, and Bayesian methods to find stable models.",
            "duration": 14,
            "difficulty": "Expert"
        },
        {
            "id": "batch_normalization",
            "title": "Batch Normalization & Internal Covariate Shift",
            "description": "Improving training stability and convergence speed using batch normalization.",
            "bulletpoints": [
                "Normalize activations using batch mean \\( \\mu \\) and variance \\( \\sigma^2 \\): \\[ \\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\]",
                "Learn scale and shift parameters \\( \\gamma, \\beta \\) to maintain model expressivity: \\[ y^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta \\]",
                "Use batch normalization to accelerate convergence and reduce dependence on initialization."
            ],
            "summaryText": "Normalize mini-batch activations with learnable scale and shift to stabilize and speed training.",
            "duration": 13,
            "difficulty": "Advanced"
        },
        {
            "id": "loss_functions",
            "title": "Loss Functions & Optimization Objectives",
            "description": "Understanding different loss functions for various machine learning tasks.",
            "bulletpoints": [
                "Minimize mean squared error (MSE) for regression: \\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]",
                "Use cross-entropy loss for classification problems: \\[ J(\\theta) = - \\sum y_i \\log(\\hat{y}_i) \\]",
                "Optimize model robustness using hinge loss in support vector machines: \\[ J(\\theta) = \\sum \\max(0, 1 - y_i w^T x_i) \\]"
            ],
            "summaryText": "Pick loss functions like MSE, cross-entropy, or hinge to align optimization with the task.",
            "duration": 12,
            "difficulty": "Advanced"
        },
        {
            "id": "learning_rate_scheduling",
            "title": "Learning Rate Scheduling & Adaptive Methods",
            "description": "Adjusting the learning rate dynamically during training.",
            "bulletpoints": [
                "Use exponential decay for adaptive learning rate scheduling: \\[ \\alpha_t = \\alpha_0 e^{-kt} \\]",
                "Implement cyclical learning rates to escape local minima.",
                "Explore optimizers like RMSProp and Adam, which adapt learning rates for each parameter."
            ],
            "summaryText": "Schedule or adapt learning rates with decay, cycles, or RMSProp/Adam to improve convergence.",
            "duration": 13,
            "difficulty": "Advanced"
        },
        {
            "id": "weight_initialization",
            "title": "Weight Initialization Strategies",
            "description": "Choosing appropriate initialization methods to avoid vanishing or exploding gradients.",
            "bulletpoints": [
                "Use Xavier (Glorot) initialization for sigmoid and tanh activations: \\[ W \\sim \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}}) \\]",
                "Apply He initialization for ReLU activations: \\[ W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}}) \\]",
                "Analyze the effects of poor initialization on training stability and convergence."
            ],
            "summaryText": "Initialize weights with Xavier or He schemes to avoid vanishing or exploding gradients.",
            "duration": 11,
            "difficulty": "Advanced"
        },
        {
            "id": "transfer_learning",
            "title": "Transfer Learning & Model Reuse",
            "description": "Leveraging pre-trained models to improve learning efficiency.",
            "bulletpoints": [
                "Fine-tune a pre-trained model by freezing lower layers and training higher layers.",
                "Use feature extraction from convolutional networks for image recognition tasks.",
                "Transfer knowledge from large datasets to smaller, domain-specific datasets."
            ],
            "summaryText": "Reuse pretrained representations and fine-tune higher layers to accelerate learning on new domains.",
            "duration": 14,
            "difficulty": "Expert"
        },
        {
            "id": "model_pruning",
            "title": "Model Pruning & Weight Quantization",
            "description": "Reducing model size while maintaining performance.",
            "bulletpoints": [
                "Remove insignificant weights using magnitude-based pruning.",
                "Use quantization to reduce model precision, improving inference speed.",
                "Optimize memory and computation requirements for edge AI deployments."
            ],
            "summaryText": "Prune low-importance weights and quantize parameters to compress models with minimal accuracy loss.",
            "duration": 12,
            "difficulty": "Advanced"
        },
        {
            "id": "adversarial_training",
            "title": "Adversarial Training & Robustness",
            "description": "Improving model resistance to adversarial attacks.",
            "bulletpoints": [
                "Generate adversarial examples using the Fast Gradient Sign Method (FGSM): \\[ x' = x + \\epsilon \\text{sign}(\\nabla_x J(\\theta, x, y)) \\]",
                "Train networks to recognize and resist perturbations in input data.",
                "Develop robust models that generalize well under adversarial conditions."
            ],
            "summaryText": "Train on FGSM-style perturbations so models withstand adversarial noise and stay robust.",
            "duration": 15,
            "difficulty": "Expert"
        }
    ]
}